---
title: "Bayesian mini-project: Mixture model for alternative splicing"
author: "Thomas Gagnieu"
date: "2025-11-05"
output: html_document
---

# Library
```{r, message=FALSE, warning=FALSE}
library(rjags)
library(coda)
library(lattice)
library(dplyr)
library(ggplot2)
library(tidyr)
library(tidyverse)
```


# Load data
```{r}
expr_df <- read.csv("data/gene_data.csv", sep = ";", header = TRUE)
splice_df <- read.csv("data/intron_data_v2.csv", sep = ";", header = TRUE)

# Prepare the merged data.frame with nmd
df <- splice_df %>%
  left_join(expr_df %>% select(gene_id, weighted_fpkm, type), by = "gene_id") %>%
  mutate(
    y = n2,                                # success
    n = n1 + n2,                           # total trials
    expr = log1p(weighted_fpkm),           # covariate log(FPKM+1)
    nmd = ifelse(type == "pseudogene", 1, 0)  # pseudogenes = 1, others = 0
  ) %>%
  filter(n > 0, !is.na(expr), y <= n)      # security screening

# Subsample for test (1000 lines)
set.seed(123)
df_small <- df %>% sample_n(1000)
df_small

# Build the list for JAGS
jags_data_mix <- list(
  y = df_small$y,
  n = df_small$n,
  expr = as.numeric(scale(df_small$expr)),                # standardisation
  introns = as.numeric(scale(df_small$splice5 + df_small$splice3)),
  N = nrow(df_small)
)
```

---

# MODELE
## Modele implementation 
```{r}
# Model JAGS (functional mixture vs background noise)

model_string_mix <- "
model {
  for (i in 1:N) {
    # Binomial data
    y[i] ~ dbin(theta[i], n[i])
    
    # Latent variable Z_i: functional or noise
    Z[i] ~ dbern(p)
    
    # Functional case: theta_f ~ Beta(a, b)
    theta_f[i] ~ dbeta(a, b)
    
    # Case background noise: logit(theta_b) = beta0 + beta1*expr + beta2*introns
    logit(theta_b[i]) <- beta0 + beta1 * expr[i] + beta2 * introns[i]
    
    # Mix
    theta[i] <- Z[i] * theta_f[i] + (1 - Z[i]) * theta_b[i]
  }

  # Priors for the betas 
  beta0 ~ dnorm(0, 0.001)
  beta1 ~ dnorm(0, 0.001)
  beta2 ~ dnorm(0, 0.001)
  
  # Prior mixture
  p ~ dunif(0, 1)

  # Beta Settings for theta_f
  a ~ dunif(0,10)
  b ~ dunif(0,10)
}
"
```


## Matrix creatrion & sampling
```{r}
# compilation
model_mix <- jags.model(
  textConnection(model_string_mix),
  data = jags_data_mix,
  n.chains = 3,
  n.adapt = 2000
)


# sampling
params_mix <- c("beta0","beta1","beta2","p","a","b","Z","theta_f") 

update(model_mix, 2000)   # additional burn-in
samps_mix <- coda.samples(model_mix, variable.names = params_mix, n.iter = 10000, thin = 10)

# convert to matrix
samps_mat_mix <- as.matrix(samps_mix)
```


## Extract and display p (posterior for the functional fraction)
```{r}
# Extract p
p_samples <- samps_mat_mix[, "p"]
p_med <- median(p_samples)
p_lo  <- quantile(p_samples, 0.025)
p_hi  <- quantile(p_samples, 0.975)

cat(sprintf("p (proportion of functional Alternative Splicing) : median = %.3f [%.3f, %.3f]\n",
            p_med, p_lo, p_hi))

# Density plot
p_df <- data.frame(p = p_samples)
ggplot(p_df, aes(x = p)) +
  geom_density(fill = "#3182bd", alpha = 0.4) +
  geom_vline(xintercept = p_med, color = "red") +
  ggtitle("Posterior de p (proportion of functional AS)") +
  xlab("p") + theme_minimal()
```


## Calculate P(Zi=1 data) and classify the introns
The columns Z[1], Z[2], ... are in samps_mat_mix. We calculate the average per column = posterior prob to be functional.
```{r}
# find out the names of the Z columns in the matrix
Z_cols <- grep("^Z\\[", colnames(samps_mat_mix), value = TRUE)
length(Z_cols)  # devrait être N


# Posterior probability P(Z_i = 1 | data)
Z_probs <- colMeans(samps_mat_mix[, Z_cols, drop = FALSE], na.rm = TRUE)


# Dataframe result with original information
res_Z <- data.frame(
  intron_idx = seq_along(Z_probs),
  P_func = Z_probs,
  gene_id = df_small$gene_id,
  expr = jags_data_mix$expr,          # standardised expr
  introns = jags_data_mix$introns
)

# display top most "functional" AS (for example P > 0.9)
head(res_Z[order(-res_Z$P_func), ], 20)
table(cut(res_Z$P_func, breaks = c(0,0.01,0.1,0.5,0.9,0.99,1), include.lowest=TRUE))


# simple classification
res_Z <- res_Z %>%
  mutate(class = case_when(
    P_func >= 0.9 ~ "Very likely functional",
    P_func >= 0.5 ~ "Likely functional",
    P_func >= 0.1 ~ "Uncertain",
    TRUE ~ "Likely noise"
  ))

# number by category
table(res_Z$class)
```


## Convergence check / diagnostic for Z and p
Checks R-hat / Gelman diagnostics and autocorr for global parameters and inspects P(Z) by chains
```{r}
# Gelman diagnostics for global settings
gelman.diag(samps_mix[, c("beta0","beta1","beta2","p","a","b")])
autocorr.plot(samps_mix[, c("beta0","beta1","beta2","p","a","b")])


# convert mcmc.list -> table (iterations x chains x variables)
# simple check: density plot of P_func on all columns
hist(Z_probs, breaks = 50, main = "Distribution of P(Z_i=1)", xlab = "P_func")


# for a particular intron (index j), look at trace by chain
j <- 1
zname <- paste0("Z[", j, "]")
# extract traces by chain
chains_list <- lapply(as.list(samps_mix), function(chain) chain[, zname])
```
Here, it takes a Multivariate psrf close to 1.1 (between 1.1 and 1.2), we are not very far but p and b 'a little' too mixed


## Posterior predictive check separated by classes
Compare the observed y/n distribution for introns classified as Probable functional vs Probable noise
```{r}
# Observed prop by class
obs_by_class <- df_small %>%
  mutate(P_func = Z_probs) %>%
  mutate(class = ifelse(P_func >= 0.9, "functional", "noise")) %>%
  group_by(class) %>%
  summarise(total_y = sum(y), total_n = sum(n), prop = total_y/total_n)

obs_by_class

# Simulate yrep conditional on MCMC samples:
set.seed(2025)
draws <- sample(seq_len(nrow(samps_mat_mix)), size = 500)  # e.g. 500 draws
sim_props_class <- sapply(draws, function(k){
  beta0 <- samps_mat_mix[k, "beta0"]
  beta1 <- samps_mat_mix[k, "beta1"]
  beta2 <- samps_mat_mix[k, "beta2"]
  p_s   <- samps_mat_mix[k, "p"]
  a_s   <- samps_mat_mix[k, "a"]
  b_s   <- samps_mat_mix[k, "b"]

  
  # theta_b for all i
  eta <- beta0 + beta1 * jags_data_mix$expr + beta2 * jags_data_mix$introns
  theta_b <- 1/(1+exp(-eta))
  
  # theta_f sampled from Beta(a_s,b_s) for each i (one draw per intron)
  theta_f <- rbeta(length(theta_b), a_s, b_s)
  
  # mix
  Z_sim <- rbinom(length(theta_b), 1, p_s)
  theta_sim <- Z_sim * theta_f + (1 - Z_sim) * theta_b
  yrep <- rbinom(length(theta_b), size = jags_data_mix$n, prob = theta_sim)
  
  # compute prop for the two groups as defined by posterior P_func > 0.9 on observed
  idx_func <- which(Z_probs >= 0.9)
  prop_func_sim <- sum(yrep[idx_func]) / sum(jags_data_mix$n[idx_func])
  idx_noise <- setdiff(seq_along(theta_b), idx_func)
  prop_noise_sim <- sum(yrep[idx_noise]) / sum(jags_data_mix$n[idx_noise])
  c(prop_func_sim = prop_func_sim, prop_noise_sim = prop_noise_sim)
})

sim_props_class_df <- t(sim_props_class)
colMeans(sim_props_class_df)
apply(sim_props_class_df, 2, quantile, probs = c(0.025, 0.5, 0.975))
```


## Interpretation & posterior predictions
```{r}
## Interpretation & posterior predictions
# Parameters of interest (global effects)
param_names_mix <- c("beta0", "beta1", "beta2", "p", "a", "b")


# Conversion to matrix
samps_mat_mix <- as.matrix(samps_mix)


# Checks the presence of parameters in the matrix
missing_params_mix <- setdiff(param_names_mix, colnames(samps_mat_mix))
if(length(missing_params_mix) > 0){
  stop("Parameters absent in samps_mat_mix: ", paste(missing_params_mix, collapse=", "))
}


# Median summary and IC95
summary_df_mix <- data.frame(
  param = param_names_mix,
  median = sapply(param_names_mix, function(p) median(samps_mat_mix[,p], na.rm=TRUE)),
  ci2.5  = sapply(param_names_mix, function(p) quantile(samps_mat_mix[,p], 0.025, na.rm=TRUE)),
  ci97.5 = sapply(param_names_mix, function(p) quantile(samps_mat_mix[,p], 0.975, na.rm=TRUE))
)
print(summary_df_mix)


# Odds ratios (to interpret directional effects)
or_df_mix <- summary_df_mix %>%
  filter(grepl("^beta", param)) %>%
  mutate(
    OR_median = exp(median),
    OR_lo = exp(ci2.5),
    OR_hi = exp(ci97.5)
  )
print(or_df_mix)


# Posterior distribution of p (proportion of functional isoforms)
p_samples <- samps_mat_mix[, "p"]
p_med <- median(p_samples)
p_lo  <- quantile(p_samples, 0.025)
p_hi  <- quantile(p_samples, 0.975)
cat(sprintf("p posterior = %.3f [%.3f, %.3f]\n", p_med, p_lo, p_hi))
```


## Posterior densities of the coefficients
```{r}
samps_df_mix <- as.data.frame(samps_mat_mix[, c("beta0","beta1","beta2", "p", "a", "b")])
samps_long_mix <- samps_df_mix %>%
  pivot_longer(everything(), names_to = "param", values_to = "value")


ggplot(samps_long_mix, aes(x = value, fill = param)) +
  geom_density(alpha = 0.4) +
  facet_wrap(~param, scales = "free") +
  theme_minimal() +
  ggtitle("Posterior densities of the coefficients")
```


## Predictive probabilities for type combinations
```{r}
# Typical covariate combinations
new_data_types <- expand.grid(
  expr = c(-1, 0, 1),
  introns = c(-1, 0, 1)
)


# Marginal predictive function for the mixture model
posterior_probs_for_row_mix <- function(new_row, samps_mat){
  eta_post <- samps_mat[, "beta0"] +
              samps_mat[, "beta1"] * new_row["expr"] +
              samps_mat[, "beta2"] * new_row["introns"]
  theta_b <- 1 / (1 + exp(-eta_post))
  theta_f_mean <- samps_mat[, "a"] / (samps_mat[, "a"] + samps_mat[, "b"])
  theta_mix <- samps_mat[, "p"] * theta_f_mean + (1 - samps_mat[, "p"]) * theta_b
  return(theta_mix)
}


# Calculation of predictive probabilities
pred_list_mix <- lapply(seq_len(nrow(new_data_types)), function(i){
  r <- new_data_types[i, ]
  mu_post <- posterior_probs_for_row_mix(as.numeric(r), samps_mat_mix)
  data.frame(
    expr = r$expr,
    introns = r$introns,
    prob_median = median(mu_post, na.rm=TRUE),
    prob_lo = quantile(mu_post, 0.025, na.rm=TRUE),
    prob_hi = quantile(mu_post, 0.975, na.rm=TRUE)
  )
})


pred_df_mix <- bind_rows(pred_list_mix)
print(pred_df_mix)
```


## Marginal effect of the expression on the 2 components
```{r warning=FALSE}
# Marginal effect for the "noise" component
expr_grid <- seq(-3, 3, length.out = 101)


# Explicit calculation according to the model: logit(theta_b) = beta0 + beta1*expr + beta2*introns
pred_grid_bruit <- lapply(expr_grid, function(e){
  eta_b <- samps_mat_mix[, "beta0"] +
           samps_mat_mix[, "beta1"] * e +
           samps_mat_mix[, "beta2"] * 0        # introns set to 0 (average)
  
  mu_b <- 1 / (1 + exp(-eta_b))                # inverse logit
  
  data.frame(
    expr = e,
    prob_median = median(mu_b, na.rm=TRUE),
    prob_lo = quantile(mu_b, 0.025, na.rm=TRUE),
    prob_hi = quantile(mu_b, 0.975, na.rm=TRUE)
  )
})


pred_df_bruit <- bind_rows(pred_grid_bruit)


ggplot(pred_df_bruit, aes(x = expr, y = prob_median)) +
  geom_line(color = "#2c7fb8", size = 1) +
  geom_ribbon(aes(ymin = prob_lo, ymax = prob_hi), fill = "#2c7fb8", alpha = 0.2) +
  ylab("Predicted probability of AS (θ_b : noise component)") +
  xlab("Expression (standardized)") +
  ggtitle("Marginal effect of the expression on the probability of minor AS\nif the AS belongs to the noise") +
  theme_minimal(base_size = 13)
```


```{r}
# Marginal effect for the "functional" component
# Esparsity of the Beta(a,b)
mu_f <- samps_mat_mix[, "a"] / (samps_mat_mix[, "a"] + samps_mat_mix[, "b"])


# As it does not depend on expr, we draw a horizontal line
pred_df_func <- data.frame(
  expr = seq(-3, 3, length.out = 101),
  prob_median = median(mu_f, na.rm=TRUE),
  prob_lo = quantile(mu_f, 0.025, na.rm=TRUE),
  prob_hi = quantile(mu_f, 0.975, na.rm=TRUE)
)


ggplot(pred_df_func, aes(x = expr, y = prob_median)) +
  geom_line(color = "#fdae61", size = 1) +
  geom_ribbon(aes(ymin = prob_lo, ymax = prob_hi), fill = "#fdae61", alpha = 0.2) +
  ylab("Predicted probability of AS (θ_f : functional component)") +
  xlab("Expression (standardized)") +
  ggtitle("Marginal effect of the expression on the probability of AS\nif the AS is functional") +
  theme_minimal(base_size = 13)
```
a and b do not depend on the expression (for the moment) so it is a horizontal line. Here, it is only the expectation of the Beta(a,b)


```{r}
pred_df_bruit$component <- "Noise"
pred_df_func$component <- "Functional"


pred_both <- bind_rows(pred_df_bruit, pred_df_func)


ggplot(pred_both, aes(x = expr, y = prob_median, color = component, fill = component)) +
  geom_line(size = 1) +
  geom_ribbon(aes(ymin = prob_lo, ymax = prob_hi), alpha = 0.2) +
  scale_color_manual(values = c("Noise" = "#2c7fb8", "Functional" = "#fdae61")) +
  scale_fill_manual(values = c("Noise" = "#2c7fb8", "Functional" = "#fdae61")) +
  ylab("Predicted probability of AS (μ)") +
  xlab("Expression (standardized)") +
  ggtitle("Comparison of noise vs functional\ncomponents according to gene expression") +
  theme_minimal(base_size = 13)
```


```{r}
# Check the observed proportion according to y used
obs_prop <- sum(jags_data_mix$y) / sum(jags_data_mix$n)
cat("Overall observed proportion (y/n) =", round(obs_prop, 6), "\n")


# Explicitly calculate the 'noise' component of the mixture model (theta_b)
expr_grid <- seq(-3, 3, length.out = 101)
pred_bruit <- lapply(expr_grid, function(e){
  eta_b <- samps_mat_mix[, "beta0"] + samps_mat_mix[, "beta1"] * e + samps_mat_mix[, "beta2"] * 0
  mu_b <- 1 / (1 + exp(-eta_b))
  data.frame(expr = e,
             median = median(mu_b, na.rm=TRUE),
             lo = quantile(mu_b, 0.025, na.rm=TRUE),
             hi = quantile(mu_b, 0.975, na.rm=TRUE))
})
pred_bruit_df <- bind_rows(pred_bruit)


# Calculate the prediction of model 2 on the same grid (if samps_mat_2 exists)
pred_m2_df <- NULL
if(exists("samps_mat_2")){
  pred_m2 <- lapply(expr_grid, function(e){
    eta2 <- samps_mat_2[, "beta0"] + samps_mat_2[, "beta1"] * e + samps_mat_2[, "beta2"] * 0
    mu2 <- 1 / (1 + exp(-eta2))
    data.frame(expr = e,
               median = median(mu2, na.rm=TRUE),
               lo = quantile(mu2, 0.025, na.rm=TRUE),
               hi = quantile(mu2, 0.975, na.rm=TRUE))
  })
  pred_m2_df <- bind_rows(pred_m2)
}


# Calculate the average functional component (theta_f mean)
mu_f_samples <- samps_mat_mix[, "a"] / (samps_mat_mix[, "a"] + samps_mat_mix[, "b"])
mu_f_med <- median(mu_f_samples)
mu_f_lo  <- quantile(mu_f_samples, 0.025)
mu_f_hi  <- quantile(mu_f_samples, 0.975)
cat("Theta_f (E[theta_f]) median =", round(mu_f_med,6), " [", round(mu_f_lo,6), ",", round(mu_f_hi,6), "]\n")


#  Display some comparative values (expr = -3, 0, +3)
for(e in c(-3,0,3)){
  b_val <- pred_bruit_df$median[which.min(abs(pred_bruit_df$expr - e))]
  cat("noise median at expr=", e, "->", round(b_val,6), "\n")
  if(!is.null(pred_m2_df)){
    m2_val <- pred_m2_df$median[which.min(abs(pred_m2_df$expr - e))]
    cat("model2 median at expr=", e, "->", round(m2_val,6), "\n")
  }
}
```
So the average proportion of observed success (y/n) ~ 2.5%, and the model says that:
- the functional component has an average probability ~ 3.4%
- the noise component ~ 0.5% -> 0.01% according to the expression
and that’s exactly what the graph shows:
The functional part is therefore 'high' (around 0.03-0.04)
The noise part is "low" (around 0.005 or less)

Empirical averages:
| component | total_y | total_n | prop |
| functional | 109156 | 2694746 | 0.0405 |
| noise | 709 | 1721394 | 0.00041 |

-> The values of the model fit perfectly:
Functional 4%
Noise 0.04%
The model well reproduces this distinction.


---


# EVALUATION
## Posterior Predictive Check global (PPC)
Objective: check if the model can reproduce the distribution of observed y/n.
```{r}
## Posterior Predictive Check global
set.seed(1234)


# Select 500 MCMC samples
draws_ppc <- sample(seq_len(nrow(samps_mat_mix)), size = 500)


# For each sample: simulate new y_rep values
ppc_df <- lapply(draws_ppc, function(k){
  beta0 <- samps_mat_mix[k, "beta0"]
  beta1 <- samps_mat_mix[k, "beta1"]
  beta2 <- samps_mat_mix[k, "beta2"]
  p_s   <- samps_mat_mix[k, "p"]
  a_s   <- samps_mat_mix[k, "a"]
  b_s   <- samps_mat_mix[k, "b"]
  
  eta <- beta0 + beta1 * jags_data_mix$expr + beta2 * jags_data_mix$introns
  theta_b <- 1/(1+exp(-eta))
  theta_f <- rbeta(jags_data_mix$N, a_s, b_s)
  Z_sim <- rbinom(jags_data_mix$N, 1, p_s)
  theta <- Z_sim * theta_f + (1 - Z_sim) * theta_b
  yrep <- rbinom(jags_data_mix$N, size = jags_data_mix$n, prob = theta)
  return(sum(yrep) / sum(jags_data_mix$n))  # simulated global proportion
})

ppc_df <- data.frame(prop_sim = unlist(ppc_df))
obs_prop <- sum(jags_data_mix$y) / sum(jags_data_mix$n)

ggplot(ppc_df, aes(x = prop_sim)) +
  geom_histogram(bins = 40, fill = "#3182bd", alpha = 0.4, color = "white") +
  geom_vline(xintercept = obs_prop, color = "red", linetype = "dashed", size = 1) +
  labs(title = "Posterior Predictive Check (overall proportion y/n)",
       x = "Proportion simulated under the model",
       y = "Frequency") +
  theme_minimal(base_size = 13)

cat("Observed proportion :", round(obs_prop,4), "\n",
    "Simulated 95% interval :", quantile(ppc_df$prop_sim, c(0.025,0.975)), "\n")
```
Observed: 0.0249
Simulated interval (95%): [0.0156, 0.0480]
The observed proportion lies within the predictive interval.
The model is therefore generally well calibrated: it correctly reproduces the total proportion of success (y/n) in the population.


## Posterior Predictive Check par classe (“noise” vs “functional”)
```{r}
## PPC par classe (functional vs noise selon Z_probs observé)
set.seed(2025)
draws <- sample(seq_len(nrow(samps_mat_mix)), size = 500)

# For each sample: simulate new yrep values and compute prop by class
ppc_class <- sapply(draws, function(k){
  beta0 <- samps_mat_mix[k, "beta0"]
  beta1 <- samps_mat_mix[k, "beta1"]
  beta2 <- samps_mat_mix[k, "beta2"]
  p_s   <- samps_mat_mix[k, "p"]
  a_s   <- samps_mat_mix[k, "a"]
  b_s   <- samps_mat_mix[k, "b"]

  eta <- beta0 + beta1 * jags_data_mix$expr + beta2 * jags_data_mix$introns
  theta_b <- 1/(1+exp(-eta))
  theta_f <- rbeta(length(theta_b), a_s, b_s)
  Z_sim <- rbinom(length(theta_b), 1, p_s)
  theta <- Z_sim * theta_f + (1 - Z_sim) * theta_b
  yrep <- rbinom(length(theta_b), size = jags_data_mix$n, prob = theta)
  
  idx_func <- which(Z_probs >= 0.9)
  idx_noise <- setdiff(seq_along(theta_b), idx_func)
  c(
    prop_func = sum(yrep[idx_func]) / sum(jags_data_mix$n[idx_func]),
    prop_noise = sum(yrep[idx_noise]) / sum(jags_data_mix$n[idx_noise])
  )
})


ppc_class_df <- as.data.frame(t(ppc_class))
obs_class <- df_small %>%
  mutate(P_func = Z_probs,
         class = ifelse(P_func >= 0.9, "functional", "noise")) %>%
  group_by(class) %>%
  summarise(obs_prop = sum(y)/sum(n))

ggplot(ppc_class_df, aes(x = prop_func)) +
  geom_histogram(bins = 40, fill = "#fdae61", alpha = 0.4, color = "white") +
  geom_vline(xintercept = obs_class$obs_prop[obs_class$class=="functional"], 
             color = "red", linetype = "dashed") +
  labs(title = "PPC – Proportion d’AS (classe fonctionnelle)", x = "Prop simulée", y = "Frequency") +
  theme_minimal(base_size = 13)

ggplot(ppc_class_df, aes(x = prop_noise)) +
  geom_histogram(bins = 40, fill = "#2c7fb8", alpha = 0.4, color = "white") +
  geom_vline(xintercept = obs_class$obs_prop[obs_class$class=="noise"], 
             color = "red", linetype = "dashed") +
  labs(title = "PPC - Proportion of AS (noise class)", x = "Proportion simulated under the model", y = "Fréquence") +
  theme_minimal(base_size = 13)
```
Functional class:
Orange histogram = model simulations for introns classified as "functional".
Observation (red line)   0.04, which falls well at the center of the simulated distribution.
The model well reproduces the functional component (θ_f3-4%).
The 'flat' behavior is consistent here, and the simulations perfectly cover reality

Noise class
Blue histogram: simulations for introns classified as noise.
The observed (0.0004) is much lower than predicted by the model (simulated distribution centered around 0.02).
So the model strongly overestimates the probability of AS from the noise.
In other words, the model believes that even "noisy" introns have a proba of AS around 2%, when in reality it is 100 lower.

Why? '
Several possible reasons:

- The noise component (θ_b) depends on the expression and introns via logistic regression:
logit(θ b) = β 0 + β 1 * expr + β 2 * introns
But β 0 is not negative enough to go down to 0.0004 even at expr=3, we get ~0.001-0.002, not 0.0004.

- The model mixes "functional" and "noise" globally via a single p.
The global "p" (0.78) means that some of the introns with low AS are still "pulled" towards θ_fvia the average of the mixture.

- θ_bis probably too rigid.
The model assumes a single logistic function for all noise, but in reality some "noisy" introns probably have an even lower probability (substructure of unmodelled noise)

-- Conceptual interpretation
The model explains the functional component well, but it does not fully explain the "zero-excess" of noise.
In other words, there is not enough mass around 0 for θ_b.
The observations have many introns "almost never spliced", and the model fails to create this "wall at zero".


## Observed y/n correlation vs posterior mean predicted
A more "frank" way of assessing the accuracy of the model on each observation.
```{r}
# Calculate the predicted average probability for each observation
theta_post_mean <- colMeans(sapply(seq_len(nrow(samps_mat_mix)), function(k){
  beta0 <- samps_mat_mix[k, "beta0"]
  beta1 <- samps_mat_mix[k, "beta1"]
  beta2 <- samps_mat_mix[k, "beta2"]
  p_s   <- samps_mat_mix[k, "p"]
  a_s   <- samps_mat_mix[k, "a"]
  b_s   <- samps_mat_mix[k, "b"]
  
  eta <- beta0 + beta1*jags_data_mix$expr + beta2*jags_data_mix$introns
  theta_b <- 1/(1+exp(-eta))
  theta_f <- a_s / (a_s + b_s)
  p_s * theta_f + (1-p_s)*theta_b
}), na.rm=TRUE)


# Observed
y_obs_prop <- jags_data_mix$y / jags_data_mix$n


# Correlation
y_obs_prop <- df_small$y / df_small$n
theta_post_mean <- colMeans(samps_mat_mix[, grep("^theta", colnames(samps_mat_mix))])
cor_val <- cor(y_obs_prop, theta_post_mean, use="pairwise.complete.obs")
cat("y_obs vs prob_pred corrélation :", round(cor_val, 3), "\n")


# Scatterplot
ggplot(data.frame(obs = y_obs_prop, pred = theta_post_mean),
       aes(x = obs, y = pred)) +
  geom_point(alpha = 0.4, color = "#3182bd") +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  scale_x_log10() + scale_y_log10() +
  labs(title = "Observed vs predicted correlation (posterior mean)",
       x = "Observed proportion y/n", y = "Predicted probability (posterior mean)") +
  theme_minimal(base_size = 13)

var_fit <- var(theta_post_mean)
var_res <- mean(apply(samps_mat_mix[,grep("^theta", colnames(samps_mat_mix))], 1, var))
R2_bayes <- var_fit / (var_fit + var_res)
cat("R² : ", R2_bayes)
```
A correlation of 0.99 indicates that:
The predicted values of the model (posterior means θ i) almost perfectly follow the observed proportions of AS in your data.
In other words:
the model captures very faithfully the structure of the observed signal,
the relationship between covariates (expr, introns, etc.) and the response (y/n) is well learned,
there are very few systematic errors.

This means that the functional vs noise mixture describes very well the observed splicing behaviors and that the introns that the model classifies as "functional" indeed have much higher observed AS rates (around 3-4%), and those classified as "noise" have rates close to zero, as expected.
So, the model well separates the two biological regimes.

The R² of 0.46 says there is a lot of dispersion around this trend:
the predictions are good on average, but with a non-negligible uncertainty at the scale of each observation.

---

# INTERPRETATION
## 1. Structure of the model

Use of a **Bayesian mixture model** separating two types of AS:

| Component              | Interpretation                                      | Parameters                                                                              |
| ----------------------- | --------------------------------------------------- | --------------------------------------------------------------------------------------- |
| **Noise (θ_b)**         | Stochastic AS / splicing errors                | depends on the expression (`expr`) and the number of introns (“introns”) via `β `, `β₁`, `β₂` |
| **Functional (θ_f)** | AS regulated / biologically useful                    | follows a Beta(a, b) -> independent of `expr`  ! LIMIT                                            |
| **p**                   | proportion of 'functional' AS in the whole | global scalar                                                                         |

---

## 2. Main parameters

| Parameters                  | Median                      | IC95%                                                                                                       | Interpretation |
| -------------------------- | ---------------------------- | ----------------------------------------------------------------------------------------------------------- | -------------- |
| **β₀ = -7.14**             | [-7.49, -6.84]               | Intercept the basic probability of AS noise very weaklye                                                  |                |
| **β₁ = -0.66**             | [-0.87, -0.47]               | Effect **negative** of the expression on the probability of AS (the more a gene is expressed, the less noise there is) |                |
| **β₂ = +0.29**             | [+0.10, +0.46]               | Effect **positive** of the number of AS: long genes have slightly more bruit                        |                |
| **p = 0.78**               | [0.72, 0.83]                 | ≈ 78% of the AS belong to the functional component                                              |                |
| **a = 0.11**, **b = 3.11** | → (E[θ_f] = a/(a+b) ≈ 0.034) | Mean AS probability for functional introns   **3.4 %*                                          |                |

---

## 3. Summary of the observed proportions

| Component            | total_y | total_n   | Empirical proportion |
| ----------------- | ------- | --------- | -------------------- |
| **Functional** | 109,156 | 2,694,746 | 0.0405 (4.0 %)       |
| **Noise**         | 709     | 1,721,394 | 0.00041 (0.04 %)     |

This contrast is **huge**: about **100 more AS in functional introns** than in those classified as noise.
The model well reproduces this difference:

* θ_f ≈ 0.034 (3.4 %)
* θ_b ≈ 0.005 → 0.0001 according to `expr`

---

## 4. Marginal effect curves (depending on the expression)

We have 2 very distinct behaviors:

| Composante           | Forme de la courbe                                      | Interprétation                                                                                                                                                      |
| -------------------- | ------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Functional** | ~ constant (flat around 0.03-0.04)                  | The "useful" alternative splicing is not related to gene expression. It remains present at a constant level.                                                          |
| **Noise**         | strongly decreasing with `expr` (from ~0.006 to <0.001) | The "noisy" splicing gradually disappears when genes are more expressed. This corresponds to a **stronger quality control** on abundant transcripts. |

---

## 5. Integrated biological interpretation

### A. The two components are clearly separated

* Functional introns form the majority ( 80%), but their AS probabilities remain moderate (~3-4%).
* "Noisy" introns are rare (~20%), and have an extremely low AS proba (<0.1%).
This suggests that a large part of the detected AS corresponds to **stable or conserved events**, not random noise.

---

### B. Expression impact

The parameter **β  = -0.66** is strongly negative:

> The more a gene is expressed, the more the probability of noisy-origin AS decreases.

This behavior is **typical of a quality control mechanism (NMD, splicing monitoring, etc.)** : highly expressed transcripts undergo less noise.

---

### C. Role of the number of introns

**β  > 0** indicates:

> Longer genes have more opportunities for splicing errors.

It is consistent with studies showing that genomic complexity favors stochastic noise.

---

### D. The flat functional curve

It is expected because we did not make it depend on `θ_f` from `expr`(Beta law)
Interpretation:
Functional AS is regulated independently of the overall expression level - it reflects specific splice programs rather than expression noise.

---

### E. Verification of the mixture

The model estimates:

> ( p 0.78 Rightarrow 78%) of the introns are functional, ( 22% ) are noise.

But the AS probabilities of the two groups differ by 100: this shows that the mixture indeed separates two distinct statistical regimes.

---

## 6. Synthetic final interpretation

| Aspect                         | Results                                                      | Interpretation                                           |
| ------------------------------ | ------------------------------------------------------------ | -------------------------------------------------------- |
| Functional proportion (`p`)    | 0.78                                                         | The majority of introns show a consistent / stable AS    |
| fonctional AS (θ_f)            | ≈ 3–4 %                                                      | Reflects biologically regulated AS                       |
| Noised AS (θ_b)                | <0.1 %                                                       | Random epissag eventse                                   |
| Effect of the expression       | Negative on the noise                                        | Highly expressed genes have fewer errors                 |
| Effect of the number of introns| Positive                                                     | More introns = more opportunities for errors             |
| Visual correlation             | Two distinct regimes (functional flat, decreasing noise)     | Clearly identifiable model                               |

---

## 7. General conclusion

> The mixture model suggests that **the alternative splicing observed in the data results from two distinct processes:**
>
> * A greatly reduced stochastic splicing noise in highly expressed genes.
> * A **functional alternative splicing**, present at a moderate level (~3-4%) and independent of the expression.
>
> This distinction allows quantifying the relative weight of noise in AS data and confirming that most observed splicing events have a signature consistent with a functional role rather than just transcription noise.
